# -*- coding: utf-8 -*-
"""BDM_HW3_sl9374.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tGcmjgEFvWZpaxWzoXTn4rEys_9v_eon

# Apache Spark - Analyzing Food Insecurity in NYC using KeyFoods Price Catalogs

In this project, we study the food insecurity problem by looking at the listed prices of various food items across neighborhoods in NYC. Our hypothesis is that *people living in areas with higher food insecurity problems would pay more for the same items compared to those in more secured areas*. For the scope of work, we will only assess food products from KeyFoods supermarkets, one of the top 4 Supermarket Leaders in Metro New York (according [Food Trade News 2021 report](https://www.foodtradenews.com/2021/06/29/food-trade-news-2021-market-study-issue/)). In particular, we will use the following datasets:

### **`keyfood_products.csv`**

This CSV file contains the price information about 2 million food items listed on KeyFoods stores in NYC.

|store|department|upc|product|size|price|
|--|--|--|--|--|--|
102|bakery|102-28556000000|Store Prepared - Challah Egg|1 EA|\$4.99 each|
102|bakery|102-28781600000|Store Prepared - fw Cheesecake Plain 7 Inch|1 EA|\$27.99 each|
|...|...|...|...|...|...|

The details of the columns are as follows:

|Column|Description|
|--|--|
|**store** | The unique id of each store |
|**department**| The department (or aisle) that the food item belongs to. Possible values are:<br />`'bakery'`,`'beverages'`,`'breakfast'`,`'deli'`,`'frozen'`,`'international'`,<br/>`'meatandseafood'`,`'pantry'`,`'produce'`,`'refrigerated'`, and `'snacks'`|
|**upc**|The unique id for each item in a food store. It is often in the format of `SID-XXXXXXXXXX`,<br/> where `SID` is a store id if it's specific to a store, `UPC` if it's a general product, or `'KEY'` <br/> if it's a KeyFoodsproduct. If an item doesn't have any UPC code, this field will be `N/A`.|
|**product**|This is the listed name of the product|
|**size**|The unit that the product is being sold in|
|**price**|The price text of the product as shown on their websites. This is not a number but have<br/>been verified to start with the price mark`$XX.XX`. Note that for items without price<br/>information, this field could be listed as `Price not Available`|



This is the *big data*, where we need to use Apache Spark to process it.

### **`keyfood_nyc_stores.json`**

This JSON file contains information for all KeyFoods stores in NYC. There are a lot of details about each store, however, for this project, we are only interested in the following fields:

|Field|Description|
|--|--|
|**name** | This is the unique id of each store, which could be crosswalk with the **store** field above |
|**communityDistrict**|The community district code that the store belongs to. It's simply a larger geographical<br/> unit comparing to a zip code. More information can be found [here](https://communityprofiles.planning.nyc.gov/).|
|**foodInsecurity**|A food insecurity score computed for the community district that the stores belong to.<br/> This value has the range of 0 to 1 with 0 being without any food insecurity rish, and 1 <br/> has the most food insecure risk.|

### **`keyfood_sample_items.csv`**

This data contains the list of 22 food items that we would like to study initially to assess our hypothesis. For each item, we have the UPC code (which needs to be generalized across store) and the item name. Here is the list:

|UPC code|Item Name|
|--|--|
|SID-20308200000|Broccoli Crowns|
|KEY-000000004094|Fresh Produce - Carrot Bunch|
|KEY-000000004062|Fresh Produce - Cucumbers|
|SID-00000004072|Fresh Produce - Potatoes Russet|
|SID-00000004131|Fresh Produce - Apples Fuji Large|
|KEY-00000004013|Produce - Orange Navel 113|
|UPC-048500001004|Tropicana - Juice Orange Pure Prem Orig|
|UPC-017400108700|Carolina - Whole Grain Brown Rice|
|UPC-016000487697|General Mills - Cherrios Multi Grain Cereal|
|UPC-073296027686|Urban Meadow - 100 Whole Wheat Bread|
|UPC-048000013651|Chicken of the Sea - Solid Wht Albacore Tuna in Oil|
|SID-20115200000|Beef - Beef Semi Bnls Chuck Stk|
|SID-28080600000|Perdue - Split Chicken Breast Fam Pack|
|UPC-073296057461|Urban Meadow - Plain Low Fat Yogurt|
|UPC-041757021443|Laughing Cow - White Cheddar Wedges|
|UPC-073296069280|Urban Meadow - Large White Eggs|
|UPC-088365000347|Cream O Land - Gallon 2% Milk|
|UPC-072940744016|Redpack - Tomato Crushed|
|UPC-051500255162|Jif - Creamy Peanut Butter|
|UPC-073296025903|Urban Meadow - Canola Oil|
|UPC-041331124461|Goya - Beans Cannelini Can|
|UPC-014500001894|Birds Eye - Spinach Leaf|

where `SID` should be replaced with the store id.

### Notes

* The big data set (`keyfood_products.csv`) is assumed to be on HDFS, and must be accessed using Spark (either as RDD or DataFrame).

* You are not allowed to collect the raw data to the notebook and process them without using Spark. However, it is okay to collect intermediate data for processing. Just try to collect as little as possible.

## Environment Setup

Since there's a number of issues with `gdown` recently, we're switching to use `curl` to download the data files.
"""

!curl -L "https://drive.google.com/uc?id=1O1U_t-cpmValVK2mjdTzcFxIbGw05vOw&confirm=t" > keyfood_sample_items.csv
!curl -L "https://drive.google.com/uc?id=1YUBKrtNV3QUz1RutMnMbJdQj7rv-Lkd5&confirm=t" > keyfood_nyc_stores.json
!curl -L "https://drive.google.com/uc?id=1f79oETtvN3NQLYPnVGhurE1UBDP4IQP-&confirm=t" > keyfood_products.csv
!pip install pyspark

# Commented out IPython magic to ensure Python compatibility.
import csv
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import IPython
# %matplotlib inline
IPython.display.set_matplotlib_formats('svg')
pd.plotting.register_matplotlib_converters()
sns.set_style("whitegrid")

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import types as T
sc = pyspark.SparkContext.getOrCreate()
spark = SparkSession(sc)
spark

"""## Task 1 - Visualizing Distributions of Listed Food Prices

In the first task, we would like to see how the listed prices for food items vary across stores. For each item in the sample list provided in `keyfood_sample_items.csv`, we can simply overlay a [strip plot](https://seaborn.pydata.org/generated/seaborn.stripplot.html) with a [violin plot](https://seaborn.pydata.org/generated/seaborn.violinplot.html). In addition, to better correlate the price distribution with the food insecurity risk, we will color the markers by the percentage of food insecurity (derived from the `foodInsecurity` field in `keyfood_nyc_stores.json`). An expected visualization is provided below:
"""

# @title

"""To produce the plot, we need to following data, where each row represents a listing of the sample food item from a store.

| Item Name	| Price ($) | % Food Insecurity |
|--|--|--|
|Urban Meadow - 100 Whole Wheat Bread | 2.29 | 11 |
|General Mills - Cherrios Multi Grain Cereal | 6.79 | 11 |
|Birds Eye - Spinach Leaf | 2.29 | 11 |
|Beef - Beef Semi Bnls Chuck Stk | 7.99 | 11 |
|Chicken of the Sea - Solid Wht Albacore Tuna in Oil | 2.49 | 11 |
| ... | ... | ... |

Your task is to compute the above table from the input data.

### INPUT
**You must read `keyfood_products.csv` into a Spark's DataFrame or Spark's RDD**, and process the data from there. You may assume the other two files (`keyfood_nyc_stores.json` and `keyfood_sample_items.csv`) are stored locally with your driver code.

### OUTPUT
Your output must be stored in another Spark's DataFrame or Spark's RDD, and being named as `outputTask1` at the end of the task. The column order must be the same as above (item name, price, and then % food insecurity). If your output is an RDD, each record should be a tuple of 3 elements. If your output is a Spark's DataFrame, it must have exactly 3 columns (you name them anything but they must in the same order).

### [TODO] A. Complete your code

Using either Spark's RDD or Spark's DataFrame transformations. The output must be placed in the `outputTask1` variable:

* Items must be filtered by UPC codes and names provided in the `keyfood_sample_items.csv`. UPC codes are considered equal if their numeric parts (the second portion after the `-`) are the same. For example, `SID-20308200000` is the same as `102-20308200000`, `KEY-20308200000`, etc.

* `Item Name` must be taken the sample items when there's a UPC code match (as defined above).

* `Price` should be extracted from the `price` column of `keyfood_products.csv`. The prefix `$` should be removed, and the output price should be converted to a float number (i.e. not a string).

*  `% Food Insecurity` is simply the percentage of the `foodInsecurity` score, i.e. by multiplying `foodInsecruity` by `100`.
"""

# Dealing with json
data_json = pd.read_json('keyfood_nyc_stores.json')
data_json=data_json.transpose()

data_json=data_json[['name','communityDistrict','foodInsecurity']]
data_json

data_json['% Food Insecurity']=(data_json['foodInsecurity']*100)
data_json

data_json['% Food Insecurity'] = data_json['% Food Insecurity'].astype(float).astype(int)
data_json['name'] = data_json['name'].astype(int)
data_json['% Food Insecurity'] = round(data_json['% Food Insecurity'])
data_json

data_json.rename(columns={'name' : 'store'},inplace=True)
data_json=data_json[['store','% Food Insecurity']]
data_json

df_json=spark.createDataFrame(data_json)

df_json.printSchema()
df_json.show()

# Dealing with sample
df_sample = spark.read.csv('/content/keyfood_sample_items.csv',header=True)\
            .withColumn('UPC code',F.trim(F.split('`UPC code`','-')[1]))

df_sample.show()

df_sample = df_sample.withColumnRenamed('UPC code', 'upc')

# Dealing with product using Spark Dataframe
df_product = spark.read.csv('/content/keyfood_products.csv',header=True)\
             .withColumn('price',F.regexp_replace(F.col('price'),"[^0-9\.]","").cast('double'))\
             .withColumn('upc',F.trim(F.split('upc','-')[1]))

df_product.show()

# Joining time
df_temp = df_product.join(df_sample,on=['upc'],how='inner')
outputTask1 = df_temp.join(df_json,on=['store'],how='inner')
outputTask1.show()

# Specifying desired columns and outputs
outputTask1 = outputTask1.withColumnRenamed('price', 'Price($)')
outputTask1 = outputTask1[['Item Name','Price($)','% Food Insecurity']]
outputTask1.show()

## DO NOT EDIT BELOW 
outputTask1 = outputTask1.cache()
outputTask1.count()

"""### B. Run to validate your output"""

#@title
def dfTask1(data):
    rdd = data.rdd if hasattr(data, 'rdd') else data
    if rdd.count()>10000:
        raise Exception('`outputTask1` has too many rows')
    rows = map(lambda x: (x[0], x[1], int(x[2])), rdd.collect())
    return pd.DataFrame(data=rows, columns=['Item Name','Price ($)','% Food Insecurity'])

def plotTask1(data, figsize=(8,8)):
    itemNames = pd.read_csv('keyfood_sample_items.csv')['Item Name']
    itemKey = dict(map(reversed,enumerate(itemNames)))
    df = dfTask1(data).sort_values(
        by = ['Item Name', '% Food Insecurity'],
        key = lambda x: list(map(lambda y: itemKey.get(y,y), x)))
    plt.figure(figsize=figsize)
    ax = sns.violinplot(x="Price ($)", y="Item Name", data=df, linewidth=0,
                        color='#ddd', scale='width', width=0.95)
    idx = len(ax.collections)
    sns.scatterplot(x="Price ($)", y="Item Name", hue='% Food Insecurity', data=df,
                    s=24, linewidth=0.5, edgecolor='gray', palette='YlOrRd')
    for h in ax.legend_.legendHandles: 
        h.set_edgecolor('gray')
    pts = ax.collections[idx]
    pts.set_offsets(pts.get_offsets() + np.c_[np.zeros(len(df)),
                                            np.random.uniform(-.1, .1, len(df))])
    ax.set_xlim(left=0)
    ax.xaxis.grid(color='#eee')
    ax.yaxis.grid(color='#999')
    ax.set_title('Item Prices across KeyFood Stores in NYC')
    return ax

if 'outputTask1' not in locals():
    raise Exception('There is no `outputTask1` produced in Task 1.A')

plotTask1(outputTask1);

